---
title: üóìÔ∏è Microphones in Operating Rooms 
date: 2023-05-29 14:12:43 +/-0800
categories: [Publication, Programming]
tags: [TCN, Surgical Phase, OR]
---

How can we automatically understand what is happening in the operating room (OR)? If we can solve this problem, we can monitor all ORs in a medical center at a glance, ensure patient safety through real-time alerts, and even create structured medical documentation automatically after each procedure‚Äîeliminating the need for physicians to manually fill out reports and thus saving their valuable time.

The most widely adopted approach for analyzing surgical workflows (or any type of complex workflow, action sequence, or procedural video) is to decompose the procedure into smaller, predefined units and recognize them sequentially as the data unfolds. This approach is commonly referred to as surgical phase recognition. For instance, in the port-catheter placement operation‚Äîwhich is the focus of our research‚Äîwe define eight such distinct surgical phases that capture the full course of the procedure.

To tackle this problem, we propose using microphones in the OR for the first time as a primary data source. The port-catheter placement is a minimally invasive intervention, which is typically performed by a single physician and a medical assistant. We collaborated with the radiology department of University Hospital Erlangen, where both physicians and assistants agreed to wear wireless headsets during procedures. Their speech was recorded continuously throughout the operation. In addition, we installed an ambient microphone (initially as part of a GoPro camera used to assist with annotation), and later included its audio as a separate ambient channel in our dataset.

Beyond capturing verbal communication, we also observed that the X-ray machine is frequently used during the procedure to verify needle or catheter placement. Therefore, we collected all X-ray images taken during each operation. Together, the speech recordings and X-ray images form our novel multimodal dataset, which we named PoCaP (Port Catheter Placement).

After collecting the data‚Äîalong with meticulous annotation and preprocessing‚Äîwe trained a phase classification model. For the feature extraction stage, we leveraged pretrained Wav2Vec for processing speech signals and TorchXrayVision for the X-ray images. These features were further refined using residual blocks inspired by the ResNet architecture. Finally, we modeled the temporal sequence of surgical phases using Temporal Convolutional Networks (TCNs), which are well-suited for sequence modeling due to their ability to capture long-range dependencies efficiently. Our source code is available on [GitHub](https://github.com/kubicndmr/PoCaPNet).

![Desktop View](/assets/img/pocap1.png){: w="800" h="600" }

One critical aspect we had to address was the imbalanced distribution of classes. In surgical workflows, certain phases‚Äîsuch as catheter insertion or positioning‚Äîtake significantly longer than others, like closing. As a result, the amount of training data available for major phases is substantially larger than for shorter, less frequent phases. This imbalance poses a challenge during model training, as it can lead to poor recognition of underrepresented phases.

To combat this issue, we employed class-imbalance-aware loss functions. Specifically, we adopted the Label-Distribution-Aware Margin (LDAM) loss, which adjusts the classification margins based on the effective number of samples per class. This approach improves the model‚Äôs ability to learn from rare classes and helped us recognize even the minor phases with meaningful accuracy.

In our baseline setup, we achieved 82.56% frame-wise accuracy and an F1-score of 81.30%. These results are promising, especially considering the noisy and spontaneous nature of surgical speech data. However, we asked ourselves: Can we do even better? The answer is yes‚Äîby fully leveraging multimodal data.

We observed that not all data modalities are equally relevant across all phases. For example, during the initial patient preparation phase, no X-ray images are taken, speech alone dominates this phase. Conversely, in phases like catheter positioning, the X-ray machine is heavily used. This insight led us to adapt our model architecture to be modality-aware.

We restructured our network into modality-specific branches: one for speech and one for image data. Their outputs are then fused in a phase-dependent manner. Additionally, we enriched our dataset with log data automatically recorded by the X-ray system, including device parameters such as position, dosimetry, and collimator settings. These logs serve as a lightweight yet informative signal to complement audio and visual inputs.

For audio fusion, we integrated Gated Multimodal Units (GMU), which allow the model to dynamically weigh the importance of different audio channels (e.g., physician vs. assistant vs. ambient) depending on the current phase. This gating mechanism ensures that the most relevant source of information is emphasized at each step of the procedure.

Altogether, these architectural and data-centric improvements enable more robust and context-sensitive recognition of surgical phases. As we continue to iterate, our goal is to build intelligent, minimally intrusive systems that support medical staff by understanding surgical workflows in real time‚Äîultimately contributing to safer, more efficient operating rooms.

![Desktop View](/assets/img/pocap2.png){: w="700" h="400" }

As expected, we achieved significantly better results with this split and modality-aware approach. Specifically, we reached 92.65% frame-wise accuracy and 92.30% F1-score, representing an improvement of approximately 10 percentage points in both metrics compared to our baseline. To gain a deeper understanding of the contribution of each data source, we conducted an ablation study by isolating individual channels and measuring their performance.

| Channel         | Accuracy (%)      | F1-Score (%)     |
|-----------------|------------------|------------------|
| Speech-All      | 93.08 ¬± 3.49     | 93.17 ¬± 3.42     |
| Only Physician  | 88.99 ¬± 2.84     | 88.62 ¬± 3.30     |
| Only Assistant  | 74.35 ¬± 7.19     | 66.77 ¬± 11.60    |
| Only Ambient    | 81.39 ¬± 6.36     | 79.73 ¬± 7.77     |
| Image-All       | 92.34 ¬± 4.60     | 91.81 ¬± 4.84     |
| Only X-ray      | 81.14 ¬± 9.05     | 76.38 ¬± 9.17     |


These results align well with our hypothesis that combining multiple modalities enhances performance. When using the full speech input (i.e., all speech channels combined), the model performs best, confirming the complementary value of different speech sources. Among individual channels, the physician channel yields the highest accuracy and F1-score, which is expected given that physicians guide most of the procedure and verbalize key actions and decisions.

In contrast, performance drops significantly when using only the assistant channel. This outcome is consistent with our observational notes from the data collection phase: medical assistants occasionally step out of the OR, communicate with other staff, or engage in tasks unrelated to the procedure. Consequently, their speech is less informative and more fragmented, contributing to increased noise and decreased predictive power.

The ambient channel, while capable of capturing all acoustic activity in the room, shows a roughly 9-point drop in F1-score compared to the physician channel. Despite its broad coverage, the ambient audio suffers from reverberation, overlapping speech, and background noise, which reduce its effectiveness as a standalone input. Nevertheless, when used in conjunction with other channels, it still provides complementary cues.

On the imaging side, the X-ray channel alone performs moderately well, but its predictive strength increases substantially when combined with structured log data from the X-ray machine. These logs‚Äîcontaining metadata such as imaging angles, exposure settings, and device positions‚Äîadd temporal and contextual information that enhances phase recognition. By integrating this auxiliary information, we observed an approximately 15-point increase in F1-score for the image model.

Together, these findings demonstrate the value of a multimodal, context-aware architecture that can intelligently select and fuse relevant data streams for each phase of surgery. By moving beyond single-channel models, we open the door to robust, real-time surgical workflow analysis that adapts to the dynamic nature of the OR.