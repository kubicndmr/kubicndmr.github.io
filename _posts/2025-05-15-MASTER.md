---
title: üóìÔ∏è Attending to a Speaker in Cocktail-Party  
date: 2020-08-29 14:12:43 +/-0800  
categories: [Publication, Programming]  
tags: [EEG, LMMSE, Bayesian, SVM]
math: true
---

![Desktop View](/assets/img/master1.png){: w="800" h="400" }

Imagine being in a (cocktail) party and having fun with friends. That's a great feeling. But we cannot listen to everyone at the same time; we typically listen to one person at a time. We can easily focus on individuals and isolate their speech with little effort among many others. Our brains are very capable and effective in solving such problems. But, what if we are using hearing aids? Will they provide the same functionality when we are in the cocktail-party? The answer is unfortunately no! These small digital signal processing (DSP) units cannot achieve the performance of the human auditory system yet. They struggle especially in noisy environments with overlapping speech, where distinguishing the speaker of interest becomes extremely challenging.

Our work starts here. We want to estimate who the person intends to listen to among multiple speech sources. This is a very complex problem. To simplify it, we make a set of assumptions and create a controlled setup. First, we assume that there are only two speech sources, i.e., two people are talking at the same time, not more. Second, we assume that hearing aids separate these speech sources perfectly, i.e., we will directly use the output of the speech sources and the input of the microphones in the hearing aids. We are aware that these assumptions do not hold true in real life, but to prove our concept we start with the simple case.

In our experimental setup, we ask participants (half of them using hearing aids) to listen to one of two different stories played on two different speakers placed equidistant at a 45-degree azimuth angle. While they are attending to one of the stories, their evoked brain activities are recorded non-invasively using 22 EEG electrodes. The task is designed to resemble natural listening conditions, yet remains well-controlled for reproducibility and clean analysis.

We are interested in the correlation between evoked brain signals and the speech sources. Theoretically, the attended speech signal will have higher correlation to EEG signals than the unattended speech signal. But typically, direct correlation computation does not yield promising results while EEG signals contain information about many other activities in the brain. We need to decode the part related to auditory activities. One thing we can do is focus on electrodes closer to brain parts related to the auditory system. More importantly, we can reconstruct speech from EEG signals or vice versa. In this way, we filter the EEG signals and extract components most relevant to speech attention. There are two approaches to solving such problems: the forward model and the backward model. We can either reconstruct EEG signals using speech sources and see which one has higher correlation, or we can reconstruct speech sources using EEG signals and see which one can be constructed more successfully.

Let's focus on the backward model and generate speech signals from EEG signals. We will make one final assumption here: the brain is a linear system, and we will estimate this temporal response function (TRF). This TRF allows us to model the transformation from neural signals back to the acoustic features of the speech, specifically its envelope, providing a way to quantify attentional focus.

![Desktop View](/assets/img/master2.png){: w="500" h="400" }

Let $$r(t)$$ be the EEG signal at time $$t$$, $$\mathbf{S}(t)$$ be the speech signal (represented as a matrix by aggregating lagged versions), and $$ \mathbf{\theta}$$ be the TRF. We want to estimate TRF first and then reconstruct $$r(t)$$. 

We will use Bayesian estimator instead of classical estimator. We assume that $$P(\theta , r)$$ is a Gaussian distribution. Thus the TRF function according to conditional normal distribution theorem is:

$$ \varepsilon (\mathbf{\theta} |¬†\mathbf{r})  = \mu_{\theta} + \Sigma_{\mathbf{\theta} r} \Sigma_{rr} (\mathbf{r}-\mu_{r}).$$

Now lets focus on each element in this equation and see how they are calculated.

First the mean of the TRF vector,

$$ \mu_{\mathbf{\theta}} = \mathcal{E} (\mathbf{\theta}) $$
 
Mean of the reconstructed EEG signal (assuming that it has a zero mean noise):

$$
\begin{equation} 
\begin{split}
\mu_{\mathbf{r}} &= \mathcal{E}(\mathbf{S}\mathbf{\theta} + \mathbf{w}) \\
&= \mathbf{S} \mu_{\theta} 
\end{split} 
\end{equation}
$$

Covariance matrix is the next: 

$$
\begin{equation} 
\begin{split}
\mathbf{\Sigma}_{\mathbf{\theta r}} &= \mathcal{E}[(\mathbf{\theta} - \mu_{\mathbf{\theta}})(\mathbf{r} - \mu_{\mathbf{r}})^T] \\ 
&= \mathcal{E}[(\mathbf{\theta} - \mu_{\mathbf{\theta}})(\mathbf{S \theta} + \mathbf{w} - \mathbf{S}\mu_{\theta})^T] \\
&= \mathcal{E}[(\mathbf{\theta} - \mu_{\mathbf{\theta}})(\mathbf{S}(\mathbf{\theta - \mu_{\mathbf{\theta}}}) + \mathbf{w} )^T] \\
&= \mathcal{E}[(\mathbf{\theta} - \mu_{\mathbf{\theta}})(\mathbf{S}(\mathbf{\theta - \mu_{\mathbf{\theta}}}))^T] \\
&= \mathbf{\Sigma}_\mathbf{\theta \theta} \mathbf{S}^T
\end{split} 
\end{equation}
$$

Finally the variance matrix of $$\mathbf{r}$$:

$$
\begin{equation} 
\begin{split}
\mathbf{\Sigma}_{\mathbf{rr}} &= \mathcal{E}[(\mathbf{r} - \mu_{\mathbf{r}})(\mathbf{r} - \mu_{\mathbf{r}})^T] \\ 
&= \mathcal{E}[(\mathbf{S \theta} + \mathbf{w} - \mathbf{S}\mu_{\theta})(\mathbf{S \theta} + \mathbf{w} - \mathbf{S}\mu_{\theta})^T] \\
&= \mathbf{S \Sigma}_{\mathbf{\theta \theta}}\mathbf{S}^T + \mathbf{\Sigma}_{\mathbf{ww}} 
\end{split} 
\end{equation}
$$

We will plug in these results into the main equation and obtain the following expression: 

$$
\begin{align*}
\mathcal{E} \{ \mathbf{\theta} | \mathbf{r} \} = \mu_{\mathbf{\theta}} + \mathbf{\Sigma}_\mathbf{\theta \theta} \mathbf{S}^T (\mathbf{S \Sigma}_{\mathbf{\theta \theta}}\mathbf{S}^T + \mathbf{\Sigma}_{\mathbf{ww}})^{-1} (\mathbf{r}- \mathbf{S}\mu_{\mathbf{\theta}}).
\end{align*}
$$

That's very good already. From this equation we only dont know the noise matrix $$ \mathbf{\Sigma}_{\mathbf{ww}}$$, and we should estimate it. There are again several ways to estimate the noise. For example, we can select the electrode we know furthest from the audiory region of the brain and compute the mean of it. We can assume that these channel has minimum amount of information related to the input speech signals. In another method, we can assume that the noise is independent identically distributed (iid) and write it as $$ \mathbf{\Sigma}_{\mathbf{ww}} = \sigma_\mathbf{w}\mathbf{I}_{N} $$. Then, we need to estimate the parameter $$ \sigma_\mathbf{w} $$. We can do this by projecting EEG signal from a selected electrode to a noise subspace. Feels like a rabbit hole with all estimations and assumptions. I'll stop here and jump to conclusion, we got the main idea already. Let's have a look at estimated TRF functions. 

![Desktop View](/assets/img/master3.png){: w="800" h="400" }

This scenario represents the ideal case and is presented here to illustrate a clear goal. When we compare the temporal response functions (TRFs) for attended and unattended speech sources, we observe that the amplitude of the attended TRF is consistently higher than that of the unattended one. This suggests that the brain effectively applies a scaling function to suppress the irrelevant (unattended) speech source‚Äîessentially multiplying it with a TRF that has smaller values. In contrast, the relevant (attended) speech source is amplified, receiving stronger weights through the TRF.

It is also important to analyze the extremum points of these TRFs. Typically, both attended and unattended TRFs exhibit a prominent positive peak around 200 ms and a negative peak near 100 ms post-stimulus. Instead of reconstructing the full speech signals and computing correlation scores‚Äîwhich is computationally expensive‚Äîwe can simply estimate both TRFs and compare their peak amplitudes. This allows for a more efficient classification of auditory attention.

In my thesis, we adopted this approach and trained support vector machines (SVMs) using features derived from these extremum points. Our experimental results showed that, within the hearing-impaired subject group, we achieved a mean classification accuracy of 60.67%. For the normal-hearing subject group, the mean accuracy was slightly lower, at 59.45%. These results demonstrate the viability of using TRF peak features as a low-complexity method for auditory attention decoding. Also, our biggest achievement was to use single electrode to achieve these results instead of all 22 electrodes as it is the case in the literature. Our source code is available on [GitHub](https://github.com/kubicndmr/Auditory-Attention-Estimation-w-EEG).