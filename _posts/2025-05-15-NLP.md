---
title: üìù My NLP Notes  
date: 2021-07-15 15:00:00 +/-0800  
categories: [Notes]  
tags: [nlp]
math: true
---

I am following the famous Stanford course cs224n and keep my notes from papers I've read here.

All 72 papers will be added soon.

---
#### **Deep contextualized word representations, 2017**
ELMo

**Problem:** Word representations should ideally model both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).

**Solution:** Generate *deep contextualized* word representations using bidirectional language models. ELMo stands for **E**mbeddings from **L**anguage **Mo**dels. Each generated token is a function of entire sentence and representations are deep compared to previous studies. 

**Model:** ELMo is a task specific combination of the intermediate layer representations of biLMs. 

Given set of tokens $$ \{ t_1, t_2, ..., t_N \}$$, a forward language model is trained to compute joint probabilty as

$$

p(t_1, t_2, ..., t_N) = \prod_{k=1}^{N} p(t_k | t_1, t_2, ..., t_{k-1}).

$$

First a context independent token vektor $$x_k$$ is computed and top layer of LSTM language model $$\overrightarrow{h}_{k,L}^{LM}$$ is used for estimating next token. For the backward langauge model, same process is applied to reversed token sequence.

To compute ELMo vectors for each token $$t_k$$, a set of $$2L + 1$$ representations are used. They are obtained from two language models (forward and backward) and context independent token vector.

$$
\mathcal{R}_k = \{ x_k^{LM}, \overrightarrow{h}_{k,j}^{LM}, \overleftarrow{h}_{k,j}^{LM} |¬†j = 0,1...,L \}
$$

where $$h_{k,j}$$ is the hidden vector of the $$j$$th layer. For each task, final weights are computed as the weighted sum of these vectors.

$$
ELMo_k^{task} = \gamma^{task} \Sigma_{j = 0}^{L} s_j^{task} h_{k,j}^{LM}
$$

where $$\gamma^{task}$$ is the task specisific parameter, $$ s_j^{task} $$ are the softmax normalized weights, and $$h_{k,j}^{LM}$$ is the concatination of $$\overrightarrow{h}_{k,j}^{LM}$$ and $$\overleftarrow{h}_{k,j}^{LM}$$. 

Considering that the activations of each biLM layer have a different distribution, in some cases layer normalization is also used.

Given a pre-trained biLM and a supervised architecture for a target NLP task, the process is simply run the biLM and record all of the layer representations for each word. Then, end task model will learn a linear combination of these representations.

---
#### **Universal Language Model Fine-tuning for Text Classification, 2018**
ULMFiT

**Problem:** Fine-tuning a language model (LM) require millions of in-domain documents to achieve good performance, which severely limits its applicability. Not the idea of LM fine-tuning but our lack of knowledge of how to train them effectively has been hindering wider adoption. 

LMs overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier. Compared to CV, NLP models are typically more shallow (at that time) and thus require different fine-tuning methods.

**Background Knowledge:** *Inductive transfer* learning involves training a model on a source task and then adapting it to a new target task. *Transductive transfer* learning involves training a model on a combination of source and target task data.

**Solution:** **U**niversal **L**anguage **M**odel **Fi**ne-**T**uning, ULMfiT leverages general-domain pretraining and novel fine-tuning techniques to prevent overfitting.

**Model:** 



---
#### **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019**

**Problem:** The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. Such restrictions are sub-optimal for sentence-level tasks.

**Background Knowledge:** There are two existing strategies for applying pre-trained language representations to downstream tasks:

<div style="display: flex; align-items: center; justify-content: space-between; gap: 40px;">

  <div style="flex: 1; border-right: 2px solid #ccc; padding-right: 14px;">
    <h4>Feature-based Approach</h4>
    <h4>ELMo</h4>
    <p>
      Uses task-specific architectures that include the pre-trained representations as additional features. It provides these features into another model.
    </p>
  </div>

  <div style="flex: 1; padding-left: 14px;">
    <h4>Fine-tuning Approach</h4>
    <h4>GPT</h4>
    <p>
      Introduces minimal task-specific parameters, and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters.
    </p>
  </div>

</div>
<br/><br/>
  
The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.

**Solution:** Improve the fine-tuning based approaches with BERT: **B**idirectional **E**ncoder **R**epresentations from **T**ransformers. 

It alleviates the unidirectionality constraint. This allows the model to capture context from both the left and the right simultaneously. Moreover, BERT enables fine-tuning with minimal task-specific parameters.

In contrast, ELMo generates contextualized word embeddings by shallowly concatenating the outputs of independently trained left-to-right and right-to-left language models, without jointly optimizing them for downstream tasks.

BERT follows a two-stage training paradigm:

1. Unsupervised pre-training on a large corpus using masked language modeling and next sentence prediction tasks.

2. Supervised fine-tuning on task-specific datasets, where all model parameters‚Äîincluding the output layer and all transformer layers‚Äîare updated jointly.

**Model:** BERT is built on a bidirectional Transformer encoder architecture, following the original implementation. The model consists of multiple Transformer blocks, where:

ùêø is the number of layers (i.e., Transformer blocks)
ùêª is the hidden size
ùê¥ is the number of self-attention heads

BERT is released in two main configurations:
- BERT<sub>BASE</sub>: 12 layers, hidden size of 768, 12 attention heads ‚Äî total 110M parameters
    
- BERT<sub>LARGE</sub>: 24 layers, hidden size of 1024, 16 attention heads ‚Äî total 340M parameters

The last hidden state of the model provides context-aware word representations.

It uses WordPiece tokenization with a fixed vocabulary of 30,000 subword tokens.

The objective function has two components:
1. Masked language model (MLM): good for token level tasks, e.g., NER
2. Next Sentence Prediction (NSP): good for sentence level tasks

The first token of every sequence is always a special classification token [CLS]. The final hidden state corresponding to this token is used as the ag- gregate sequence representation for classification tasks. [SEP] is a special token for seperating sentences in NSP task. [MASK] token is used in MLM task.
